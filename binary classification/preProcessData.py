# -*- coding: utf-8 -*-
"""
Created on Wed Sep 25 11:09:46 2019

This module pre-processes feature vectors obtained from CNN in previous module  
and tweak them into format required for neural neteork.

Step1: 
    Prepare target_run1, target_run2, target_run3 and so on...
Step2: 
    Load all convolutional feature vectors into np_feature_AllRuns.
Step3: 
    Standardize dataset and apply PCA on it.
    
Run Instructions:
=================
    python preProcessData.py
    
Output Files:
=============
    All output files are stored in "data" folder.
    
    Set1 - Target Variables:
    -----------------------    
    {"target_run1.npy", "target_run2.npy", "target_run3.npy", "target_run4.npy"}
     -  These files have labels of images corresponding to each run in a foramt
        suitable for training LSTM or GRU network.
        
    Set2 - Feature Vectors of each run:
    -----------------------------------
    {v2_ResNet_npyD_Run1.npy, v2_ResNet_npyD_Run2.npy and so on...}
    -   Each file has concolutional feature vectors of all images corresponding
        to a given run. 
        Naming Convention: (featureFolder[0:-8]+'_Run'+runName[-1]+'.npy')
        
    Set3 - PCA Transforms:
    ----------------------
    {'scaler.pickle', 'pca.pickle'}
    -   These are files obtained after standardizing the dataset and after 
        applying PCA respectively.
        
    Set4 - PCA Outputs:
    -------------------
    {PCA_v2_ResNet_npyD_Run1.npy, PCA_v2_ResNet_npyD_Run2.npy and so on...}
    -   These are the files obtained by  applying principal component analysis
        on set2 files.
        Naming Convention: 'PCA_'+(featureFolder[0:-8]+'_Run'+runName[-1]+'.npy')
        
    Set5 - LSTM/GRU formatted input:
    --------------------------------
    {LSTMfmt_v2_ResNet_npyD_Run1.npy, LSTMfmt_v2_ResNet_npyD_Run2.npy and son on...}
    -   Files obtained by modifying PCA Outputs(Set4 files) to a format suitable for 
        LSTM or GRU networks.
        Naming Convention: 'LSTMfmt_'+(featureFolder[0:-8]+'_Run'+runName[-1]+'.npy')
        
    Set6 - Final Train and Test Files:
    ----------------------------------
    {LSTM_x_Train.npy, LSTM_y_Train.npy, LSTM_x_Test.npy, LSTM_y_Test.npy}
    -   These are the final train and test files which will be used in next module
        for training and testing our model.
        

@author: cgudaval
"""

# =============================================================================
# Import necessary libraries
# =============================================================================
import os
import argparse
import pandas as pd
import numpy as np
import pickle

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

#%%
# =============================================================================
# Process Input Arguments
# =============================================================================
parser = argparse.ArgumentParser(
        description='This module pre-processes feature vectors obtained from CNN in '
                    'previous module and tweak them into format required for neural '
                    'network.')
parser.add_argument('--windowsize', help='windowTime = (windowSize/2) minutes',\
                    default=12, required=False, type=int)
parser.add_argument('--pcavariance', help='Variance to be retained in input dataset', \
                    default=0.70, required=False, type=float)
parser.add_argument('--featurefolder', help='Name of the folder in which convolutional feature vectors are stored',\
                    default='v2_ResNet_npyData_Crop', required=False, type=str)
parser.add_argument('--path_data', help='Location of folders containing run folders. Eg: "FY18_LT_baseline_1"',\
                    default=os.getcwd(), required=False, type=str)
args = parser.parse_args()

# =============================================================================
# Hyper Parameters to be tuned
# =============================================================================
windowSize = args.windowsize #12 
pcaVariance = args.pcavariance #0.70

# =============================================================================
# Initializations
# =============================================================================
featureFolder = args.featurefolder #'v2_ResNet_npyData_Crop' #'v2_VGG_npyData_Crop' #'densenet_npyData'
path_data = args.path_data #os.getcwd() #os.path.join("C:/","Users","cgudaval", "Desktop", "FCIC", "FCIC_data")

runs = ['FY18_LT_baseline_1','FY18_LT_baseline_2','FY18_LT_baseline_3','FY18_LT_baseline_4']
imLabelPath = os.path.join(path_data,'data',"imLabels_FD_60.csv") #Location of csv file generated by "imagetagger_baseline.py"

#%%
# =============================================================================
# Load imLabels and prepare target arrays
# =============================================================================

print("************************************************************")
print("Preparing target_run1, target_run2, target_run3 and so on...")
print("************************************************************")

# Load imLabels file to a dataframe
df = pd.read_csv(imLabelPath, index_col = 'Unnamed: 0', infer_datetime_format  = True)
df.index = pd.to_datetime(df.index)
df['run'], df['path'] = df['path'].str.split('/',1).str
df_AllRuns = df[['run', 'path', 'anomaly']]

#Prepare target variables
target_run1 = np.array(df_AllRuns.loc[df_AllRuns['run'] == runs[0]]['anomaly'])*1
target_run2 = np.array(df_AllRuns.loc[df_AllRuns['run'] == runs[1]]['anomaly'])*1
target_run3 = np.array(df_AllRuns.loc[df_AllRuns['run'] == runs[2]]['anomaly'])*1
target_run4 = np.array(df_AllRuns.loc[df_AllRuns['run'] == runs[3]]['anomaly'])*1

target_run1 = target_run1[windowSize-1:target_run1.shape[0]]
target_run2 = target_run2[windowSize-1:target_run2.shape[0]]
target_run3 = target_run3[windowSize-1:target_run3.shape[0]]
target_run4 = target_run4[windowSize-1:target_run4.shape[0]]

np.save(os.path.join(os.getcwd(),'data','target_run1.npy'), target_run1)
np.save(os.path.join(os.getcwd(),'data','target_run2.npy'), target_run2)
np.save(os.path.join(os.getcwd(),'data','target_run3.npy'), target_run3)
np.save(os.path.join(os.getcwd(),'data','target_run4.npy'), target_run4)

#%%
# =============================================================================
# Load all CNN features to numpy arrays
# =============================================================================

print("*****************************************************************")
print("Loading all convolutional feature vectors into np_feature_AllRuns")
print("*****************************************************************")

np_feature_AllRuns = None
for runName in runs:
    if not os.path.isfile(os.path.join(path_data,'data',(featureFolder[0:-8]+'_Run'+runName[-1]+'.npy'))):
        df = df_AllRuns.loc[df_AllRuns['run'] == runName]
        
        np_feature_Master = None
        np_feature = None
        count = 0
        
        for index, row in df.iterrows():
            folders = row['path'].split('/')
            folders.append(folders[2][0:-3]+'npy')
            folders[2] = featureFolder
            featureVector = np.load(os.path.join(path_data,runName,*folders))
            featureVector = featureVector.flatten()
            try:
                np_feature = np.vstack((np_feature,featureVector))
            except:
                np_feature = featureVector
            count+=1
            if count%500 == 0:
                try:
                    np_feature_Master = np.vstack((np_feature_Master,np_feature))
                except:
                    np_feature_Master = np_feature
                print('np_feature_Master.shape:',np_feature_Master.shape)
                np_feature = None
            #print(index)
        np_feature_Master = np.vstack((np_feature_Master,np_feature))    
        np.save(os.path.join(path_data,'data',(featureFolder[0:-8]+'_Run'+runName[-1]+'.npy')), np_feature_Master)
        print("Saved to:",os.path.join(path_data,'data',(featureFolder[0:-8]+'_Run'+runName[-1]+'.npy')))
    else:
       np_feature_Master = np.load(os.path.join(path_data,'data',(featureFolder[0:-8]+'_Run'+runName[-1]+'.npy'))) 
    try:
        np_feature_AllRuns = np.vstack((np_feature_AllRuns,np_feature_Master))
    except:
        np_feature_AllRuns = np_feature_Master
    print('Completed loading'+runName+'records.')

#np.save(os.path.join(path_data,'data','np_feature_AllRuns.npy'), np_feature_AllRuns)
#np_feature_AllRuns = np.load(os.path.join(path_data, 'data', 'np_feature_AllRuns.npy'))

# =============================================================================
# Apply PCA for the WHOLE dataset
# =============================================================================
print('Applying PCA...')

for i in range(2):
    np.random.shuffle(np_feature_AllRuns) #Randomize the records before applying PCA

scaler = StandardScaler()
np_feature_AllRuns = scaler.fit_transform(np_feature_AllRuns) # Normalise np_feature_AllRuns Dataset before PCA

pca = PCA(pcaVariance) # Make an instance of the Model
temp = pca.fit_transform(np_feature_AllRuns[0:20000]) # Apply PCA on np_feature_AllRuns dataset

#Save "pca" and "scaler"
with open(os.path.join(os.getcwd(),'data','pca.pickle'), 'wb') as handle:
    pickle.dump(pca, handle, protocol=pickle.HIGHEST_PROTOCOL)

with open(os.path.join(os.getcwd(),'data','scaler.pickle'), 'wb') as handle:
    pickle.dump(scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)

##Logic to load "pca.pickle" and "scaler.pickle" 
#with open(os.path.join(os.getcwd(),'pca.pickle'), 'rb') as pickle_file:
#    pca = pickle.load(pickle_file)
#    
#with open(os.path.join(os.getcwd(),'scaler.pickle'), 'rb') as pickle_file:
#    scaler = pickle.load(pickle_file)

#Sanity check to verify PCA
#print('Shape before PCA:',np_feature_AllRuns.shape)
#np_feature_AllRuns = pca.transform(np_feature_AllRuns)
#print('Shape after PCA:',np_feature_AllRuns.shape)

# =============================================================================
# Fit each run individually with "pca" that has been obtained
# =============================================================================
featureNPYfileNames = [(featureFolder[0:-8]+'_Run'+runName[-1]+'.npy') for runName in runs]

featureVectors = dict()
for fileName in featureNPYfileNames:
    NPYpath = os.path.join(path_data, 'data', fileName)
    featureVectors[fileName] = np.load(NPYpath)
    print(fileName,': Shape before PCA:',featureVectors[fileName].shape)
    featureVectors[fileName] = pca.transform(featureVectors[fileName])#Apply the prviously obtained PCA to each run
    print(fileName,': Shape after PCA:',featureVectors[fileName].shape)
    np.save(os.path.join(path_data, 'data', 'PCA_'+fileName), featureVectors[fileName])

# =============================================================================
# LOGIC TO LOAD ALL RUNS FROM SAVED NPY FILES
# =============================================================================
#featureNPYfileNames = ['v2_VGG_Run1.npy', 'v2_VGG_Run2.npy', 'v2_VGG_Run3.npy', 'v2_VGG_Run4.npy']
#np_feature_AllRuns = None
#
#for fileName in featureNPYfileNames:
#    NPYpath = os.path.join(path_data, fileName)
#    np_feature_Master = np.load(NPYpath)
#    try:
#        np_feature_AllRuns = np.vstack((np_feature_AllRuns,np_feature_Master))
#    except:
#        np_feature_AllRuns = np_feature_Master
#    print(np_feature_AllRuns.shape)

# =============================================================================
#  Modify 'featureVectors' data into a format suitable to LSTM or GRU
# =============================================================================
for fileName in featureNPYfileNames:
    X_train = featureVectors[fileName]
    trainMaster = np.array([np.zeros(X_train[0:windowSize].shape)])
    train = np.array([np.zeros(X_train[0:windowSize].shape)])
    for rowIndex in range(windowSize,X_train.shape[0]):
        #print(fileName+':', rowIndex,'/',X_train.shape[0],'.....',trainMaster.shape)
        train = np.append(train, [(X_train[rowIndex-windowSize:rowIndex])], axis = 0)
        if (rowIndex %500) == 0:
            trainMaster = np.append(trainMaster, train[1:train.shape[0]], axis = 0)
            train = [np.zeros(X_train[0:windowSize].shape)]
    trainMaster = np.append(trainMaster, train[1:train.shape[0]], axis = 0)
    np.save(os.path.join(path_data, 'data', 'LSTMfmt_'+fileName), trainMaster)
    print('Saved to:',os.path.join(path_data, 'data', 'LSTMfmt_'+fileName))
    
LSTMfmtFiles = ['LSTMfmt_'+fileName for fileName in featureNPYfileNames]

# =============================================================================
# Train_Val_Test Split
# =============================================================================

# Method 1:
# ========

LSTMfmt_v2_ResNet_Run1 = np.load(os.path.join(os.getcwd(),'data', LSTMfmtFiles[0]))
LSTMfmt_v2_ResNet_Run2 = np.load(os.path.join(os.getcwd(),'data', LSTMfmtFiles[1]))
LSTMfmt_v2_ResNet_Run3 = np.load(os.path.join(os.getcwd(),'data', LSTMfmtFiles[2]))
LSTMfmt_v2_ResNet_Run4 = np.load(os.path.join(os.getcwd(),'data', LSTMfmtFiles[3]))

X_train = np.vstack((LSTMfmt_v2_ResNet_Run1[0:3000], LSTMfmt_v2_ResNet_Run1[4500:],\
                     LSTMfmt_v2_ResNet_Run2[0:3000], LSTMfmt_v2_ResNet_Run2[4500:],\
                     LSTMfmt_v2_ResNet_Run3[0:3000], LSTMfmt_v2_ResNet_Run3[4500:],\
                     LSTMfmt_v2_ResNet_Run4[0:3000], LSTMfmt_v2_ResNet_Run4[4500:]\
                     ))

X_test = np.vstack((LSTMfmt_v2_ResNet_Run1[3000:4500], \
                     LSTMfmt_v2_ResNet_Run2[3000:4500], \
                     LSTMfmt_v2_ResNet_Run3[3000:4500], \
                     LSTMfmt_v2_ResNet_Run4[3000:4500] \
                     ))

target_run1 = np.load(os.path.join(os.getcwd(),'data', "target_run1.npy"))
target_run2 = np.load(os.path.join(os.getcwd(),'data', "target_run2.npy"))
target_run3 = np.load(os.path.join(os.getcwd(),'data', "target_run3.npy"))
target_run4 = np.load(os.path.join(os.getcwd(),'data', "target_run4.npy"))

y_train = np.concatenate([target_run1[0:3000], target_run1[4500:],\
                         target_run2[0:3000], target_run2[4500:],\
                         target_run3[0:3000], target_run3[4500:],\
                         target_run4[0:3000], target_run4[4500:]\
                         ])

y_test = np.concatenate([target_run1[3000:4500], \
                         target_run2[3000:4500], \
                         target_run3[3000:4500], \
                         target_run4[3000:4500] \
                         ])

np.save(os.path.join(os.getcwd(), 'data', 'LSTM_x_Train.npy'), X_train)
np.save(os.path.join(os.getcwd(), 'data', 'LSTM_y_Train.npy'), y_train)
    
np.save(os.path.join(os.getcwd(), 'data', 'LSTM_x_Test.npy'), X_test)
np.save(os.path.join(os.getcwd(), 'data', 'LSTM_y_Test.npy'), y_test)

# =============================================================================
# Method-2
# =============================================================================
#
## =============================================================================
## Divide dataset into chunks
## =============================================================================
#count = 0
#LSTMinpDict = dict()
#LSTMtargetDict = dict()
#for fileName in LSTMfmtFiles:
#    runData = np.load(os.path.join(os.getcwd(),fileName))
#    targetData = np.load(os.path.join(os.getcwd(),'target_run'+fileName[-5]+'.npy'))
#    for i in range(5):
#        count += 1
#        LSTMinpDict[count] = runData[i*1273:(i+1)*1273]
#        LSTMtargetDict[count] = targetData[i*1273:(i+1)*1273]
#        print("Prepared chunk",count)
#
#list_keys = list(LSTMtargetDict.keys())
#import random
#random.shuffle(list_keys)
#
#x_Train = None
#y_Train = None
#for i in range(12):
#    try:
#        x_Train = np.append(x_Train, LSTMinpDict[list_keys[i]], axis = 0)
#        y_Train = np.append(y_Train, LSTMtargetDict[list_keys[i]])
#    except:
#        x_Train = LSTMinpDict[list_keys[i]]
#        y_Train = LSTMtargetDict[list_keys[i]]
#    print(x_Train.shape, y_Train.shape)
#np.save('LSTM_x_Train.npy', x_Train)
#np.save('LSTM_y_Train.npy', y_Train)
#    
#x_Test = None
#y_Test = None
#for i in range(12,16):
#    try:
#        x_Test = np.append(x_Test, LSTMinpDict[list_keys[i]], axis = 0)
#        y_Test = np.append(y_Test, LSTMtargetDict[list_keys[i]])
#    except:
#        x_Test = LSTMinpDict[list_keys[i]]
#        y_Test = LSTMtargetDict[list_keys[i]]
#    print(x_Test.shape, y_Test.shape)
#np.save('LSTM_x_Test.npy', x_Test)
#np.save('LSTM_y_Test.npy', y_Test)
#    
#x_Val = None
#y_Val = None
#for i in range(16,20):
#    try:
#        x_Val = np.append(x_Val, LSTMinpDict[list_keys[i]], axis = 0)
#        y_Val = np.append(y_Val, LSTMtargetDict[list_keys[i]])
#    except:
#        x_Val = LSTMinpDict[list_keys[i]]
#        y_Val = LSTMtargetDict[list_keys[i]]
#    print(x_Val.shape, y_Val.shape)
#np.save('LSTM_x_Val.npy', x_Val)
#np.save('LSTM_y_Val.npy', y_Val)

## =============================================================================
## Method-3
## =============================================================================
#X_train = None; X_test = None;
#y_train = None; y_test = None;
#
#from sklearn.model_selection import train_test_split
#for fileName in LSTMfmtFiles[0:-1]:
#    runData = np.load(os.path.join(os.getcwd(),fileName))
#    targetData = np.load(os.path.join(os.getcwd(),'target_run'+fileName[-5]+'.npy'))
#    
#    run_X_train, run_X_test, run_y_train, run_y_test = train_test_split(runData, targetData, test_size=0.33, random_state=42)
#    print("Processing",fileName)
#    try:
#        print(y_train.shape, run_y_train.shape)
#        X_train = np.vstack((X_train,run_X_train))
#        X_test = np.vstack((X_test,run_X_test))
#        y_train = np.concatenate([y_train,run_y_train])
#        y_test = np.concatenate([y_test,run_y_test])
#    except:
#        print('Exception Found')
#        X_train = run_X_train
#        X_test = run_X_test
#        y_train = run_y_train
#        y_test = run_y_test
#
################################################################################
#fileName = LSTMfmtFiles[-1]
#
#X_new = np.load(os.path.join(os.getcwd(),fileName))
#y_new = np.load(os.path.join(os.getcwd(),'target_run'+fileName[-5]+'.npy'))
#
################################################################################
#
#
#
###############################################################################

# Method-1 (VGG Specific)
# =======================

#LSTMfmt_v2_VGG_Run1 = np.load("LSTMfmt_v2_VGG_npyD_Run1.npy")
#LSTMfmt_v2_VGG_Run2 = np.load("LSTMfmt_v2_VGG_npyD_Run2.npy")
#LSTMfmt_v2_VGG_Run3 = np.load("LSTMfmt_v2_VGG_npyD_Run3.npy")
#LSTMfmt_v2_VGG_Run4 = np.load("LSTMfmt_v2_VGG_npyD_Run4.npy")
#
#X_train = np.vstack((LSTMfmt_v2_VGG_Run1[0:3000], LSTMfmt_v2_VGG_Run1[4500:],\
#                     LSTMfmt_v2_VGG_Run2[0:3000], LSTMfmt_v2_VGG_Run2[4500:],\
#                     LSTMfmt_v2_VGG_Run3[0:3000], LSTMfmt_v2_VGG_Run3[4500:],\
#                     LSTMfmt_v2_VGG_Run4[0:3000], LSTMfmt_v2_VGG_Run4[4500:]\
#                     ))
#
#X_test = np.vstack((LSTMfmt_v2_VGG_Run1[3000:4500], \
#                     LSTMfmt_v2_VGG_Run2[3000:4500], \
#                     LSTMfmt_v2_VGG_Run3[3000:4500], \
#                     LSTMfmt_v2_VGG_Run4[3000:4500] \
#                     ))
#
#target_run1 = np.load("target_run1.npy")
#target_run2 = np.load("target_run2.npy")
#target_run3 = np.load("target_run3.npy")
#target_run4 = np.load("target_run4.npy")
#
#y_train = np.concatenate([target_run1[0:3000], target_run1[4500:],\
#                         target_run2[0:3000], target_run2[4500:],\
#                         target_run3[0:3000], target_run3[4500:],\
#                         target_run4[0:3000], target_run4[4500:]\
#                         ])
#
#y_test = np.concatenate([target_run1[3000:4500], \
#                         target_run2[3000:4500], \
#                         target_run3[3000:4500], \
#                         target_run4[3000:4500] \
#                         ])
#
#np.save('LSTM_x_Train.npy', X_train)
#np.save('LSTM_y_Train.npy', y_train)
#    
#np.save('LSTM_x_Test.npy', X_test)
#np.save('LSTM_y_Test.npy', y_test)
###############################################################################
